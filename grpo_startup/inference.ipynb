{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import json\n",
    "from inspect import signature\n",
    "\n",
    "HF_TOKEN = \n",
    "model_name = \"JesseLiu/qwen25-7b-pagerank\"\n",
    "SPECIAL_TOKENS = [\"<degd>\", \"<ddd>\", \"<decgd>\", \"<demgd>\", \"<debgd>\", \"<dppd>\", \"<dpd>\"]\n",
    "\n",
    "class TokenDecoderWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        print(signature(tokenizer.batch_decode).parameters)\n",
    "\n",
    "        print(\"skip_special_tokens\" in signature(tokenizer.batch_decode).parameters)\n",
    "        self._accepts_skip = \"skip_special_tokens\" in signature(tokenizer.batch_decode).parameters\n",
    "\n",
    "    def batch_decode(self, seqs, **kw):\n",
    "        if self._accepts_skip:\n",
    "\n",
    "            \n",
    "            kw.setdefault(\"skip_special_tokens\", False)\n",
    "            return self.tokenizer.batch_decode(seqs, **kw)\n",
    "        else:\n",
    "            \n",
    "            return [self.tokenizer.decode(s, clean_up_tokenization_spaces=False, **kw)\n",
    "                    for s in seqs]\n",
    "\n",
    "    def decode(self, seq, **kw):\n",
    "        kw.setdefault(\"skip_special_tokens\", False)\n",
    "        return self.tokenizer.decode(seq, **kw)\n",
    "    def __len__(self):\n",
    "        return len(self.tokenizer)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.tokenizer(*args, **kwargs)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.tokenizer, name)\n",
    "\n",
    "raw_tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "raw_tokenizer.pad_token = raw_tokenizer.eos_token\n",
    "raw_tokenizer.padding_side = \"right\"\n",
    "raw_tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n",
    "tokenizer = TokenDecoderWrapper(raw_tokenizer)  # ✅ 替代\n",
    "\n",
    "print(\"Special tokens in tokenizer:\", tokenizer.special_tokens_map_extended)\n",
    "\n",
    "# ── model ─────────────────────────────────────────────────────────────────────\n",
    "def load_base_and_merge(adapter_repo: str, tokenizer):\n",
    "    cfg = json.load(open(hf_hub_download(adapter_repo, \"adapter_config.json\", token=HF_TOKEN)))\n",
    "    base = AutoModelForCausalLM.from_pretrained(cfg[\"base_model_name_or_path\"],\n",
    "                                                device_map=\"auto\", token=HF_TOKEN)\n",
    "    base.resize_token_embeddings(len(tokenizer))\n",
    "    merged = PeftModel.from_pretrained(base, adapter_repo, token=HF_TOKEN,\n",
    "                                       is_trainable=True).merge_and_unload()\n",
    "    return merged\n",
    "\n",
    "def is_lora_repo(repo=model_name) -> bool:\n",
    "    try:\n",
    "        hf_hub_download(repo, \"adapter_config.json\", token=HF_TOKEN)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if is_lora_repo():\n",
    "    model = load_base_and_merge(model_name, tokenizer)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 device_map=\"auto\", token=HF_TOKEN)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_emb = model.get_input_embeddings()\n",
    "    for t in SPECIAL_TOKENS:\n",
    "        tid = tokenizer.convert_tokens_to_ids(t)\n",
    "        input_emb.weight[tid] = torch.randn_like(input_emb.weight[0])\n",
    "\n",
    "\n",
    "prompt = \"Question: Is dermatitis an indication for Eflornithine?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# generate\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# decode（保留特殊标记）\n",
    "output_text = tokenizer.batch_decode(output)[0]\n",
    "print(\"=== Generation Result ===\")\n",
    "print(output_text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
